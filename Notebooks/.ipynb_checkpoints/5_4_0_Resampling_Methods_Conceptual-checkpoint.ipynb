{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Resampling Methods\n",
    "- **Chapter 5 from the book [An Introduction to Statistical Learning](https://www.statlearning.com/).**\n",
    "- **By Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani.**\n",
    "- **Pages from $197$ to $198$**\n",
    "- **By [$\\text{Mosta Ashour}$](https://www.linkedin.com/in/mosta-ashour/)**\n",
    "\n",
    "\n",
    "**Exercises:**\n",
    "- **[1.](#1)**\n",
    "- **[2.](#2) [(a)](#2a) [(b)](#2b) [(c)](#2c) [(d)](#2d) [(e)](#2e) [(f)](#2f) [(g)](#2g) [(h)](#2h)**\n",
    "- **[3.](#3) [(a)](#3a) [(b)](#3b)**\n",
    "- **[4.](#4)**\n",
    "\n",
    "# <span style=\"font-family:cursive;color:#0071bb;\"> 5.4 Exercises </span>\n",
    "## <span style=\"font-family:cursive;color:#0071bb;\"> Conceptual </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### $1.$ Using basic statistical properties of the variance, as well as single-variable calculus, derive $(5.6)$. In other words, prove that $\\alpha$ given by $(5.6)$ does indeed minimize $\\mathrm{Var}(\\alpha X + (1 − \\alpha)Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the following variance [rules](http://www.kaspercpa.com/statisticalreview.htm):**\n",
    "\n",
    "$\n",
    "\\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y) + 2 Cov(X,Y)\n",
    "\\\\\n",
    "\\text{Var}(cX) = c^2 Var(X)\n",
    "\\\\\n",
    "\\text{So}\\dots\n",
    "\\\\\n",
    "\\text{Var}(cX+dY) = c^2 \\text{Var}(X) + d^2 \\text{Var}(Y) + 2cd Cov(X,Y)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\text{thus:}\\\\\n",
    "f(\\alpha) = \\text{Var}(\\alpha X + (1 - \\alpha)Y)\\\\\n",
    "\\\\\n",
    "f(\\alpha) = \\alpha^2 \\text{Var}(X) + (1 - \\alpha)^2 \\text{Var}(Y) + 2 \\alpha (1 - \\alpha) Cov(X, Y)\\\\\n",
    "\\\\\n",
    "f(\\alpha) = \\alpha^2 \\sigma_X^2 + (1 - \\alpha)^2 \\sigma_Y^2 + 2 (\\alpha-\\alpha^2 ) \\sigma_{XY}\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\text{Take the first derivative:}\\\\\n",
    "\\frac {d} {d\\alpha} f(\\alpha) &= 0\\\\\n",
    "\\\\\n",
    "2 \\alpha \\sigma_X^2 + 2 (1 - \\alpha) (-1) \\sigma_Y^2 + 2 (1 - 2 \\alpha ) \\sigma_{XY} &= 0\\\\\n",
    "\\\\\n",
    "\\alpha \\sigma_X^2 + (\\alpha - 1) \\sigma_Y^2 + (-2 \\alpha + 1) \\sigma_{XY} &= 0\\\\\n",
    "\\\\\n",
    "\\color{red}{\\alpha \\sigma_X^2} + \\color{red}{\\alpha\\sigma_Y^2} - \\sigma_Y^2 \\color{red}{-2 \\alpha \\sigma_{XY}} + \\sigma_{XY} &= 0\\\\\n",
    "\\\\\n",
    "\\alpha (\\sigma_X^2 + \\sigma_Y^2 - 2 \\sigma_{XY}) - \\sigma_Y^2 + \\sigma_{XY} &= 0\\\\\n",
    "\\\\\n",
    "\\alpha (\\sigma_X^2 + \\sigma_Y^2 - 2 \\sigma_{XY}) &= \\sigma_Y^2 - \\sigma_{XY}\\\\\n",
    "\\\\\n",
    "\\text{therefore:}\n",
    "\\\\\n",
    "\\alpha &= \\frac {\\sigma_Y^2 - \\sigma_{XY}}\n",
    "               {\\sigma_X^2 + \\sigma_Y^2 - 2 \\sigma_{XY}}\n",
    "\\\\\n",
    "\\text{As required.}\\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### $2.$ We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of $n$ observations.\n",
    "\n",
    "<a id='2a'></a>\n",
    "**$(a)$ What is the probability that the first bootstrap observation is not the $j$th observation from the original sample? Justify your answer.**\n",
    "\n",
    ">- **Answer:**\n",
    ">  - $p(n_{1} \\neq n_j) = \\frac{n-1}{n}$\n",
    ">  - Which all the others observations are possible except just the first observation \"1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "**$(b)$ What is the probability that the second bootstrap observation is not the $j$th observation from the original sample?**\n",
    "\n",
    ">- **Answer:**\n",
    ">  - The same as the previous question:\n",
    ">  - $p(n_{2} \\neq n_j) = \\frac{n-1}{n}$\n",
    ">  - Which all the others observations are possible except just the second observation \"1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2c'></a>\n",
    "**$(c)$ Argue that the probability that the $j$th observation is not in the bootstrap sample is $(1 - 1/n)^n$.**\n",
    "\n",
    ">- **Answer:**\n",
    ">  - The probability that the $j$th observation is not in the bootstrap sample is \n",
    ">  - $(1−1/n)_1⋅(1−1/n)_2⋅(1−1/n)_3\\dots⋅(1−1/n)_n$\n",
    ">  - which is: $(1−1/n)^n$\n",
    ">  - Since choosing any sample is independent of another but has the same probability value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2d'></a>\n",
    "**$(d)$ When $n = 5$, what is the probability that the $j$th observation is in the bootstrap sample?**\n",
    "\n",
    ">- **Answer:**\n",
    ">  - when $n = 5$ and it is **in** the bootstrap sample:\n",
    ">  - $p = 1 - (1-1/5)^5$\n",
    ">  - $p = 0.67232$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2e'></a>\n",
    "**$(e)$ When n = 100, what is the probability that the $j$th observation is in the bootstrap sample?**\n",
    "\n",
    ">- **Answer:**\n",
    ">  - when $n = 100$ and it is **in** the bootstrap sample:\n",
    ">  - $p = 1 - (1-1/100)^{100}$\n",
    ">  - $p = 0.63397$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2f'></a>\n",
    "**$(f)$ When $n = 10,000$, what is the probability that the $j$th observation is in the bootstrap sample?**\n",
    "\n",
    ">- **Answer:**\n",
    ">  - when $n = 10,000$ and it is **in** the bootstrap sample:\n",
    ">  - $p = 1 - (1-1/10000)^{10000}$\n",
    ">  - $p = 0.63213$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2g'></a>\n",
    "**$(g)$ Create a plot that displays, for each integer value of $n$ from $1$ to $100,000$, the probability that the $j$th observation is in the bootstrap sample. Comment on what you observe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def prob_j_in_sample(n):\n",
    "    return 1 - (1 - 1/n)**n\n",
    "\n",
    "x = np.arange(1, 100000)\n",
    "y = np.array([prob_j_in_sample(n) for n in x])\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.lineplot(x=x, y=prob_j_in_sample(x))\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('probability')\n",
    "plt.ylim((0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- **Comments:**\n",
    ">  - We are seeing a visual representation of $\\lim\\limits_{x \\to \\infty} 1 - (1 - \\frac{1}{n})^n = 1 - \\frac{1}{\\epsilon} \\approx 0.632$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2h'></a>\n",
    "**$(h)$ We will now investigate numerically the probability that a bootstrap sample of size $n = 100$ contains the $j$th observation. Here $j = 4$. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.**\n",
    "```\n",
    "> store=rep (NA , 10000)\n",
    "> for (i in 1:10000) {\n",
    "store[i]=sum(sample (1:100 , rep =TRUE)==4) >0\n",
    "}\n",
    "> mean(store)\n",
    "```\n",
    "**Comment on the results obtained.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6335633563356335"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = [] \n",
    "for i in np.arange(1, 10000):\n",
    "    store += [np.sum((np.random.randint(low=1, high=101, size=100) == 4)) > 0]\n",
    "\n",
    "np.mean(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- **Comments:**\n",
    ">  - If we created more bootstrap samples in $h$, the result observed from a numerical approach above will approximately equal to our probabilistic estimation for a sample size of $100$ in $2.e$ which was $p = 0.63397$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3. We now review $\\text{k-fold}$ cross-validation.\n",
    "\n",
    "<a id='3a'></a>\n",
    "**$(a)$ Explain how $\\text{k-fold}$ cross-validation is implemented.**\n",
    ">- **Answer:**\n",
    ">- This approach involves randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. \n",
    ">- The first $k-1$ folds is treated as a validation set, and the remaining fold as test set.\n",
    ">- The model is then fitted to each of these folds, and testing on the remaining fold.\n",
    ">- This procedure is repeated $k$ times; each time, a different group of observations.\n",
    ">- This process results in k estimates of the test error,\n",
    "$\\text{MSE_1},\\text{MSE_2}, \\dots ,\\text{MSE_k}$. The k-fold CV estimate is computed by averaging these values,\n",
    "$$ CV(k) =\\frac{1}{k} \\sum_{i=1}^k MSE_i. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3b'></a>\n",
    "**$(b)$ What are the advantages and disadvantages of k-fold cross- validation relative to:**\n",
    "\n",
    "**$i.$ The validation set approach?**\n",
    "\n",
    "- **Answer:**\n",
    "\n",
    ">- $\\text{Advantages:}$\n",
    ">  - The validation set approach is **conceptually simple** and is **easy** to implement.\n",
    ">  - We can use the validation set approach in testing which predictor's transformations provides even better results, as we randomly split the observations into $k$ approximately equal sets or folds, a training set of k-1 folds, and the remaining fold is the validation set. \n",
    ">  - The validation set error rates that result from fitting various regression models on the training sample and evaluating their performance on the validation sample, using $\\text{MSE}$ as a measure of validation set error, comparing the $\\text{MSE}$ results will lead us to the better predictor's transformations to use in our model.\n",
    "\n",
    ">- $\\text{Disadvantages:}$\n",
    ">  - The validation estimate of the test error rate can be **highly variable**, depending on precisely which observations are included in the training and validation sets.\n",
    ">  - Only a subset of the observations those that are included in the training set rather than in the validation set are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to **overestimate the test error rate** for the model fit on the entire data set.\n",
    ">  - The approach has a computational advantage: a model is trained and tested once. In $k-fold$ CV, $k$ models will be trained, and for standard values of $k$. Means that $k-fold$ CV can be far more computationally expensive for large dataset and for large values of $k$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**$ii.$ LOOCV?**\n",
    "\n",
    "- **Answer:**\n",
    "\n",
    ">- $\\text{Advantages:}$\n",
    ">  - It has far less bias over the validation set approach.\n",
    ">  - Performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.\n",
    ">  - $\\text{Leave-one-out cross-validation}$ involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation $(x_1, y_1)$ is used for the validation set, and the remaining observations ${(x_2, y_2), \\dots , (x_n, y_n)}$ make up the training set. The statistical learning method is fit on the $n - 1$ training observations, and a prediction $\\hat{y}_1$ is made for the excluded observation, using its value $x_1$. Since $(x_1, y_1)$ was not used in the fitting process, $\\text{MSE}_1 = (y_1 - \\hat{y}_1)^2$ provides an approximately unbiased estimate for the test error.\n",
    "\n",
    ">- $\\text{Disadvantages:}$\n",
    ">  - Even though $\\text{MSE}_1$ is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation $(x_1, y_1)$.\n",
    ">  - LOOCV requires fitting the statistical learning method $n$ times. This has the potential to be computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### 4. Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\n",
    "\n",
    ">- **Answer:**\n",
    ">  - Which we need to estimate the standard deviation from a given $n$ observations of the population. We could might estimate the standard deviation of our prediction by using the [sample standard deviation](https://en.wikipedia.org/wiki/Standard_deviation#Corrected_sample_standard_deviation) formula:\n",
    "$$\n",
    "\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^{n}{(\\hat{y_i} - \\bar{y})^2}}{n - 1}}\n",
    "$$\n",
    ">  - The accuracy of this estimate is limited by its variability, so... \n",
    ">  - We could improve the accuracy of this estimate by using the bootstrap approach. This works by randomly select $n$ observations from the original dataset to create a $B$ different bootstrap datasets. \n",
    ">  - This procedure is repeated $B$ times for some large value of $B$, in order to produce $B$ different bootstrap datasets, $Z^{*1}, Z^{*2}, \\dots , Z^{*B},$ and $B$ corresponding $\\alpha$ estimates, $\\hat{α}^{∗1}, \\hat{α}^{∗2}, \\dots , \\hat{α}^{∗B}$. We can compute the standard error of these bootstrap estimates using the formula in $(5.8)$:\n",
    "$$\n",
    "SE_B(\\hat{\\alpha}) =\n",
    "\\sqrt{\n",
    "\\frac{1}{B - 1}\n",
    "\\sum^B_{r=1}\n",
    "\\bigg(\\hat{\\alpha}^{*r} - \n",
    "\\frac{1}{B}\n",
    "\\sum^B_{r'=1}\n",
    "\\hat{\\alpha}^{*r}\n",
    "\\bigg)^2\n",
    "}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
