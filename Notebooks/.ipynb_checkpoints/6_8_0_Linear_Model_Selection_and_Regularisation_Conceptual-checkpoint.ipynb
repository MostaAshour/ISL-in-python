{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Linear Model Selection and Regularization\n",
    "- **Chapter 6 from the book [An Introduction to Statistical Learning](https://www.statlearning.com/).**\n",
    "- **By Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani.**\n",
    "- **Pages from $259$ to $262$**\n",
    "- **By [Mosta Ashour](https://www.linkedin.com/in/mosta-ashour/)**\n",
    "\n",
    "\n",
    "**Exercises:**\n",
    "- **[1.](#1) [(a)](#1a) [(b)](#1b) [(c)](#1c)**\n",
    "- **[2.](#2) [(a)](#2a) [(b)](#2b) [(c)](#2c)**\n",
    "- **[3.](#3) [(a)](#3a) [(b)](#3b) [(c)](#3c) [(d)](#3d) [(e)](#3e)**\n",
    "- **[4.](#4) [(a)](#4a) [(b)](#4b) [(c)](#4c) [(d)](#4d) [(e)](#4e)**\n",
    "- **[5.](#5) [(a)](#5a) [(b)](#5b) [(c)](#5c) [(d)](#5d)**\n",
    "- **[6.](#6) [(a)](#6a) [(b)](#6b)**\n",
    "- **[7.](#7) [(a)](#7a) [(b)](#7b) [(c)](#7c) [(d)](#7d) [(e)](#7e)**\n",
    "\n",
    "# <span style=\"font-family:cursive;color:#0071bb;\"> 6.8 Exercises </span>\n",
    "## <span style=\"font-family:cursive;color:#0071bb;\"> Conceptual </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### $1.$ We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain $p + 1$ models, containing $0, 1, 2, \\dots , p$ predictors. Explain your answers:\n",
    "\n",
    "\n",
    "<a id='1a'></a>\n",
    "**$(a)$ Which of the three models with $k$ predictors has the smallest training $\\text{RSS}$?**\n",
    "\n",
    ">**Answer:**\n",
    ">- **Best subset selection** will produce the smallest training $\\text{RSS}$ because all possible combinations of predictors are considered for a given $k$. \n",
    ">- It is **possible** that the most effective model is also found by forward stepwise or backward stepwise, but **not possible** that they will find a model more effective than best subset, as the forward stepwise and backward stepwise methods determine models with a path depending on which predictors they pick first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1b'></a>\n",
    "**$(b)$ Which of the three models with $k$ predictors has the smallest test $\\text{RSS}$?**\n",
    "\n",
    ">**Answer:**\n",
    ">- **Best subset selection** may provide the smallest test $\\text{RSS}$.\n",
    ">- It is entirely possible for forward stepwise or backward stepwise selection to provide the most effective model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1c'></a>\n",
    "**$(c)$ True or False:**\n",
    "\n",
    "**$i.$ The predictors in the $k$-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.**\n",
    "\n",
    "> **True.**\n",
    "\n",
    "**$ii.$ The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the $(k+1)$-variable model identified by backward stepwise selection.**\n",
    "\n",
    "> **True.**\n",
    "\n",
    "**$iii.$ The predictors in the $k$-variable model identified by backward stepwise are a subset of the predictors in the $(k+1)$-variable model identified by forward stepwise selection.**\n",
    "\n",
    "> **False.**\n",
    "> Forwards and backwards stepwise selections have different starting points (the null model and the full model) and will take different selection paths. The statement could be true for specific examples, but it is not generally true.\n",
    "\n",
    "**$iv.$ The predictors in the $k$-variable model identified by forward stepwise are a subset of the predictors in the $(k+1)$-variable model identified by backward stepwise selection.**\n",
    "\n",
    "> **False.** (Same as in $iii.$)\n",
    "\n",
    "**$v.$ The predictors in the $k$-variable model identified by best subset are a subset of the predictors in the $(k+1)$-variable model identified by best subset selection.**\n",
    "\n",
    "> **False.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2. For parts $(a)$ through $(c)$, indicate which of $i.$ through $iv.$ is correct. Justify your answer.\n",
    "\n",
    "<a id='2a'></a>\n",
    "**$(a)$ The lasso, relative to least squares, is:**\n",
    "\n",
    "**$i.$ More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.**\n",
    "\n",
    "> **Incorrect.** \n",
    "\n",
    "**$ii.$ More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.**\n",
    "\n",
    "> **Incorrect.**\n",
    "\n",
    "**$iii.$ Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.**\n",
    "\n",
    "> **Correct.** Least squares uses all features, whereas lasso will either use all features or set coefficients to zero for some features, thus lasso is less flexible. As $\\lambda$ increase, the variance of the model will decrease quickly at the cost of a small increase in bias resulting in improved test $\\text{MSE}$. \n",
    "\n",
    "**$iv.$ Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.**\n",
    "\n",
    "> **Incorrect.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "**$(b)$ Repeat (a) for ridge regression relative to least squares.**\n",
    "\n",
    "**$i.$ More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.**\n",
    "\n",
    "> **Incorrect.** (Same as for lasso)\n",
    "\n",
    "**$ii.$ More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.**\n",
    "\n",
    "> **Incorrect.** (Same as for lasso)\n",
    "\n",
    "**$iii.$ Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.**\n",
    "\n",
    "> **Correct.** (Same as for lasso)\n",
    "\n",
    "**$iv.$ Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.**\n",
    "\n",
    "> **Incorrect**. (Same as for lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2c'></a>\n",
    "**$(c)$ Repeat (a) for non-linear methods relative to least squares.**\n",
    "\n",
    "**$i.$ More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.**\n",
    "\n",
    "> **Incorrect.**\n",
    "\n",
    "**$ii.$ More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.**\n",
    "\n",
    "> **Correct.** Non-linear methods are more flexible than least squares. \n",
    "> Non-linear methods will lead the model to overfit the data at which point increase increased variance will begin to outweigh any further reduction in bias.\n",
    "\n",
    "**$iii.$ Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.**\n",
    "\n",
    "> **Incorrect.**\n",
    "\n",
    "**$iv.$ Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.**\n",
    "\n",
    "> **Incorrect.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3. Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$$\n",
    "\\sum^n_{i=1} \\Bigg( y_i - \\beta_0 - \\sum^p_{j=1} \\beta_j x_{ij} \\Bigg)^2 \\\n",
    "\\text{subject to} \\\n",
    "\\sum_{j=1}^p | \\beta_j | \\leq s\n",
    "$$\n",
    "\n",
    "### for a particular value of $s$. For parts $(a)$ through $(e)$, indicate which of $i.$ through $v.$ is correct. Justify your answer.\n",
    "\n",
    "\n",
    "<a id='3a'></a>\n",
    "**$(a)$ As we increase $s$ from $0$, the training $\\text{RSS}$ will:**\n",
    "\n",
    "- **$i.$ Increase initially, and then eventually start decreasing in an inverted U shape.**\n",
    "- **$ii.$ Decrease initially, and then eventually start increasing in a U shape.**\n",
    "- **$iii.$ Steadily increase.**\n",
    "- **$iv.$ Steadily decrease.**\n",
    "- **$v.$ Remain constant.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$iv.$ Steadily decrease.**\n",
    ">- As we increase $s$ we increase the models flexibility, thus will lead to overfit the training dataset with steadily decrease in the training $\\text{RSS}$ error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3b'></a>\n",
    "**$(b)$ Repeat (a) for test $\\text{RSS}$.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$ii.$ Decrease initially, and then eventually start increasing in a U shape.**\n",
    ">- The increase in the model flexibility leads to improves our model performance in the test set up to some point where our model starts to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3c'></a>\n",
    "**$(c)$ Repeat $(a)$ for variance.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$iii.$ Steadily increase.**\n",
    ">- When the constraint region increasing in size, $\\lambda$ tend to decrease, so the model flexibility increases.\n",
    ">- Thus as the model flexibility increases, the variance steadily increase. \n",
    ">- When $s$ is sufficiently large, $\\hat{\\beta}$ falls within the constraint region, and the variance will not longer increase, because the $\\hat{\\beta}$ chosen will always be the least squares estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3d'></a>\n",
    "**$(d)$ Repeat $(a)$ for (squared) bias.**\n",
    "> **Answer:**\n",
    ">- **$iv.$ Steadily decrease.**\n",
    ">- As the model flexibility increases, the bais will steadily decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3e'></a>\n",
    "**$(e)$ Repeat $(a)$ for the irreducible error.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$v.$ Remain constant.**\n",
    ">- The irreducible errors is an error that we cannot reduce by choosing a better model. it is due to randomness or natural variability in a system, thus it remains constant regardless of the model flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### $4.$ Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\Bigg( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\Bigg)^2 + \\\n",
    "\\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "### for a particular value of $\\lambda$. For parts $(a)$ through $(e)$, indicate which of $i.$ through $v.$ is correct. Justify your answer.\n",
    "\n",
    "\n",
    "<a id='4a'></a>\n",
    "**$(a)$ As we increase $\\lambda$ from 0, the training $\\text{RSS}$ will:**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$iii.$ Steadily increase.**\n",
    ">- The tuning parameter $\\lambda$ serves to control the relative impact on the regression coefficient estimates.\n",
    ">- When $\\lambda$ = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates.\n",
    ">- However, as $\\lambda \\to \\infty$, the impact of the shrinkage penalty grows, thus the training $\\text{RSS}$ will steadily increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4b'></a>\n",
    "**$(b)$ Repeat $(a)$ for test $\\text{RSS}$.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$ii.$ Decrease initially, and then eventually start increasing in a U shape.**\n",
    ">- When $\\lambda = 0$, the variance is high but there is no bias. \n",
    ">- But as $\\lambda$ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias.\n",
    ">- At some point the test $\\text{RSS}$ will start to increasing again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4c'></a>\n",
    "**$(c)$ Repeat $(a)$ for variance.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$iv.$ Steadily decrease.**\n",
    ">- When $\\lambda = 0$, the variance is high but there is no bias. \n",
    ">- But as $\\lambda$ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4d'></a>\n",
    "**$(d)$ Repeat $(a)$ for (squared) bias.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$iii.$ Steadily increase.**\n",
    ">- When $\\lambda = 0$, the variance is high but there is no bias. \n",
    ">- But as $\\lambda$ increases, the shrinkage of the ridge coefficient estimates leads to a slight increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4e'></a>\n",
    "**$(e)$ Repeat $(a)$ for the irreducible error.**\n",
    "\n",
    "> **Answer:**\n",
    ">- **$v.$ Remain constant.**\n",
    ">-  Irreducible error remains constant regardless of the model flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "### $5.$ It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.\n",
    "\n",
    "### Suppose that $n = 2, p = 2, x_{11} = x_{12}, x_{21} = x_{22}$. Furthermore, suppose that $y_1+y_2 =0$ and $x_{11}+x_{21} =0$ and $x_{12}+x_{22} =0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat{\\beta}_0 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5a'></a>\n",
    "**$(a)$ Write out the ridge regression optimization problem in this setting.**\n",
    " \n",
    "**Answer:**\n",
    "- the regression ridge regression coefficient estimates $\\hat{\\beta}^R$ are the values that minimize\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\Bigg( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\Bigg)^2 + \\\n",
    "\\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "- $\\text{With } \\hat{\\beta}_0 = 0$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\Bigg( y_i - \\sum_{j=1}^p \\beta_j x_{ij} \\Bigg)^2 + \\\n",
    "\\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "$\\text{With } y_1 + y_2 = 0$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\Bigg( - \\sum_{j=1}^p \\beta_j x_{ij} \\Bigg)^2 + \\\n",
    "\\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "$\\text{With } x_{i1} = x_{i2}$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\Bigg( - \\beta_1 x_{i1} - \\beta_2 x_{i1}\\Bigg)^2 + \\\n",
    "\\lambda (\\beta_1^2 + \\beta_2^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\Bigg(  \\beta_1^2 x_{i1}^2 + \\beta_2^2 x_{i1}^2 + 2 \\beta_1 \\beta_2 x_{i1}^2  \\Bigg) + \\\n",
    "\\lambda (\\beta_1^2 + \\beta_2^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bigg(\\sum_{i=1}^n x_{i1}^2 \\bigg) \\bigg(  \\beta_1^2 + \\beta_2^2 + 2 \\beta_1 \\beta_2  \\bigg) + \\\n",
    "\\lambda (\\beta_1^2 + \\beta_2^2)\n",
    "$$\n",
    "\n",
    "$\\text{With } x_{i1} + x_{i1} = 0$:\n",
    "\n",
    "$$\n",
    "\\big( \\beta_1^2 + \\beta_2^2 + 2 \\beta_1 \\beta_2 \\big) + \\\n",
    "\\lambda (\\beta_1^2 + \\beta_2^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\beta_1^2 + \\beta_2^2 + 2 \\beta_1 \\beta_2 + \\\n",
    "\\lambda\\beta_1^2 + \\lambda\\beta_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5b'></a>\n",
    "**$(b)$ Argue that in this setting, the ridge coefficient estimates satisfy $\\hat{\\beta}_1 = \\hat{\\beta}_2$.**\n",
    "\n",
    "**Answer:**\n",
    "- By minimizing the previous cost function for $\\beta_1$ and $\\beta_2$:\n",
    "- For $\\hat{\\beta_1}$:\n",
    "\n",
    "$$2\\lambda\\beta_1 + 2\\beta_1 + 2\\beta_2\\ = 0$$\n",
    "$$\\lambda\\beta_1 + \\beta_1 + \\beta_2 = 0$$\n",
    "\n",
    "- for $\\hat{\\beta_2}$: \n",
    "\n",
    "$$2\\lambda\\beta_2 + 2\\beta_2 + 2\\beta_1\\ = 0$$\n",
    "$$\\lambda\\beta_2 + \\beta_2 + \\beta_1 = 0$$\n",
    "\n",
    "**So:**\n",
    "\n",
    "$$\\lambda\\beta_1 + \\beta_1 + \\beta_2 = \\lambda\\beta_2 + \\beta_2 + \\beta_1$$\n",
    "$$\\lambda\\beta_1 = \\lambda\\beta_2$$\n",
    "$$\\beta_1 = \\beta_2$$\n",
    "\n",
    "**Thus, $\\hat{\\beta_1} = \\hat{\\beta_2}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5c'></a>\n",
    "**$(c)$ Write out the lasso optimization problem in this setting.**\n",
    "\n",
    "**Answer:**\n",
    "- From the form in $(a)$, the lasso optimization will be as following:\n",
    "\n",
    "$$\n",
    "\\text{RSS} + \\lambda\\sum_{j=1}^p |\\beta_j| = \\\n",
    "\\beta_1^2 + \\beta_2^2 + 2 \\beta_1 \\beta_2 + \\\n",
    "\\lambda|\\beta_1| + \\lambda|\\beta_2|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5d'></a>\n",
    "**$(d)$ Argue that in this setting, the lasso coefficients $\\beta_1$ and $\\beta_2$ are not unique--in other words, there are many possible solutions to the optimization problem in $(c)$. Describe these solutions.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    ">- Here is a geometric interpretation of the solutions for the equation in c above. We use the alternate form of Lasso constraints $| \\hat{\\beta}_1 | + | \\hat{\\beta}_2 | < s$.\n",
    ">- The Lasso constraint take the form $| \\hat{\\beta}1 | + | \\hat{\\beta}2 | < s$, which when plotted take the familiar shape of a diamond centered at origin $(0, 0)$. Next consider the squared optimization constraint $(y_1 - \\hat{\\beta}1x{11} - \\hat{\\beta}2x{12})^2 + (y_2 - \\hat{\\beta}1x{21} - \\hat{\\beta}2x{22})^2$. We use the facts $x{11} = x{12}$, $x_{21} = x_{22}$, $x_{11} + x_{21} = 0$, $x_{12} + x_{22} = 0$ and $y_1 + y_2 = 0$ to simplify it to\n",
    ">- Minimize: $2.(y_1 - (\\hat{\\beta}_1 + \\hat{\\beta}2)x{11})^2$.\n",
    ">- This optimization problem has a simple solution: $\\hat{\\beta}_1 + \\hat{\\beta}2 = \\frac{y_1}{x{11}}$. This is a line parallel to the edge of Lasso-diamond $\\hat{\\beta}_1 + \\hat{\\beta}_2 = s$. Now solutions to the original Lasso optimization problem are contours of the function $(y_1 - (\\hat{\\beta}_1 + \\hat{\\beta}2)x{11})^2$ that touch the Lasso-diamond $\\hat{\\beta}_1 + \\hat{\\beta}_2 = s$. Finally, as $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$ very along the line $\\hat{\\beta}_1 + \\hat{\\beta}2 = \\frac{y_1}{x{11}}$, these contours touch the Lasso-diamond edge $\\hat{\\beta}_1 + \\hat{\\beta}_2 = s$ at different points. As a result, the entire edge $\\hat{\\beta}_1 + \\hat{\\beta}_2 = s$ is a potential solution to the Lasso optimization problem!\n",
    ">- Similar argument can be made for the opposite Lasso-diamond edge: $\\hat{\\beta}_1 + \\hat{\\beta}_2 = -s$.\n",
    ">- Thus, the Lasso problem does not have a unique solution. The general form of solution is given by two line segments:\n",
    ">- $\\hat{\\beta}_1 + \\hat{\\beta}_2 = s;$\\\n",
    ">- $\\hat{\\beta}_1 \\geq 0;$\\\n",
    ">- $\\hat{\\beta}_2 \\geq 0$\\\n",
    ">- and $\\hat{\\beta}_1 + \\hat{\\beta}_2 = -s;$\\\n",
    ">- $\\hat{\\beta}_1 \\leq 0; \\hat{\\beta}_2 \\leq 0$\n",
    "\n",
    ">- And by minimizing the previous cost function for $\\beta_1$ and $\\beta_2$:\n",
    "$$\n",
    "\\frac{\\partial j}{\\partial \\beta_1} = \\\n",
    "2\\beta_1 + 2\\beta_2 + \\lambda = 0 \\\n",
    "\\\\\n",
    "\\frac{\\partial j}{\\partial \\beta_2} = \\\n",
    "2\\beta_1 + 2\\beta_2 + \\lambda = 0 \\tag{equivalence}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "### $6.$ We will now explore $(6.12)$ and $(6.13)$ further.\n",
    "\n",
    "<a id='6a'></a>\n",
    "**$(a)$ Consider $(6.12)$ with $p = 1$. For some choice of $y_1$ and $\\lambda > 0$, plot $(6.12)$ as a function of $\\beta_1$. Your plot should confirm that (6.12) is solved by $(6.14)$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y = 6\n",
    "lambd  = 1\n",
    "beta_1 = np.array(np.arange(-10, 10, 0.25))\n",
    "func   = (y - beta_1)**2 + lambd * (beta_1**2)\n",
    "betaR  = y/(1+lambd)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=beta_1, y=func)\n",
    "plt.axvline(x=betaR, color='r')\n",
    "plt.xlabel('beta_1')\n",
    "plt.ylabel('Ridge optimization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- The red line shows that function is indeed minimized at $\\beta = y / (1 + \\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6b'></a>\n",
    "**$(b)$ Consider $(6.13)$ with $p = 1$. For some choice of $y_1$ and $\\lambda > 0$, plot $(6.13)$ as a function of $\\beta_1$. Your plot should confirm that $(6.13)$ is solved by $(6.15)$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_1 > lambda/2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lasso(y1, β1, λ):\n",
    "    return (y1 - β1)**2 + lambd * (np.absolute(beta_1))\n",
    "\n",
    "y_1 = 3\n",
    "lambd  = 2\n",
    "beta_1 = np.array(np.arange(-5, 5, 0.15))\n",
    "func   = (y_1 - beta_1)**2 + lambd * (np.absolute(beta_1))\n",
    "\n",
    "if y_1 > lambd/2:\n",
    "    print('y_1 > lambda/2')\n",
    "    beta_L = y_1 - lambd/2\n",
    "\n",
    "if y_1 < -lambd/2:\n",
    "    print('y_1 < -lambda/2')\n",
    "    beta_L = y_1 + lambd/2\n",
    "\n",
    "if np.absolute(y_1) <= lambd/2:\n",
    "    print('np.absolute(y1) <= lambda/2')\n",
    "    beta_L = 0\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=beta_1, y=func)\n",
    "plt.axvline(x=beta_L, color='r')\n",
    "plt.xlabel('beta_1')\n",
    "plt.ylabel('Lasso optimization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- The red line shows that function is indeed minimized at $\\beta = y - \\lambda / 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "### $7.$ We will now derive the Bayesian connection to the lasso and ridge regression discussed in Section $6.2.2$.\n",
    "\n",
    "### $\\color{red}{\\text{Note: }}$ This exercise is marked as being particularly hard by the authors, and I couldn't do it by myself. [Here](https://github.com/asadoughi/stat-learning/blob/master/ch6/7.md) is a great solution provided by *Amir Sadoughi and josh Davis*.\n",
    "\n",
    "<a id='7a'></a>\n",
    "### (Copied)\n",
    "**$(a)$ Suppose that $y_i = \\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j+\\epsilon_i$ where $\\epsilon_1, . . . , \\epsilon _n$ are independent and identically distributed from a $N(0, \\sigma^2)$ distribution. Write out the likelihood for the data.**\n",
    "\n",
    "**Answer:**\\\n",
    "The likelihood for the data is:\n",
    "\n",
    "$$ \\begin{aligned} L(\\theta \\mid \\beta) &= p(\\beta \\mid \\theta) \\\\ \n",
    "&= p(\\beta_1 \\mid \\theta) \\times \\cdots \\times p(\\beta_n \\mid \\theta) \\\\\n",
    "&= \\prod_{i = 1}^{n} p(\\beta_i \\mid \\theta) \\\\ \n",
    "&= \\prod_{i = 1}^{n} \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\exp \\left(- \\frac{ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) }{ 2\\sigma^2 } \\right) \\\\ \n",
    "&= \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 \\right) \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7b'></a>\n",
    "**$(b)$ Assume the following prior for $\\beta: \\beta_1, . . . , \\beta_p$ are independent and identically distributed according to a double-exponential distribution with mean 0 and common scale parameter $b$: i.e. $p(\\beta) = \\frac{1}{2b} exp(-|\\beta|/b)$. Write out the posterior for \\beta in this setting.**\n",
    "\n",
    "**Answer:**\\\n",
    "The posterior with double exponential (Laplace Distribution) with mean 0 and common scale parameter $b$, i.e. $p(\\beta) = \\frac{1}{2b}\\exp(- \\lvert \\beta \\rvert / b)$ is:\n",
    "\n",
    "$$ f(\\beta \\mid X, Y) \\propto f(Y \\mid X, \\beta) p(\\beta \\mid X) = f(Y \\mid X, \\beta) p(\\beta) $$\n",
    "\n",
    "Substituting our values from (a) and our density function gives us:\n",
    "\n",
    "$$ \\begin{aligned} f(Y \\mid X, \\beta)p(\\beta) &= \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 \\right) \\left( \\frac{ 1 }{ 2b } \\exp(- \\lvert \\beta \\rvert / b) \\right) \\\\ \n",
    "&= \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ 2b } \\right) \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 - \\frac{ \\lvert \\beta \\rvert }{ b } \\right) \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7c'></a>\n",
    "**$(c)$ Argue that the lasso estimate is the mode for $\\beta$ under this posterior distribution.**\n",
    "\n",
    "**Answer:**\\\n",
    "Showing that the Lasso estimate for $\\beta$ is the mode under this posterior distribution is the same thing as showing that the most likely value for $\\beta$ is given by the lasso solution with a certain $\\lambda$.\n",
    "\n",
    "We can do this by taking our likelihood and posterior and showing that it can be reduced to the canonical Lasso Equation 6.7 from the book.\n",
    "\n",
    "Let's start by simplifying it by taking the logarithm of both sides:\n",
    "\n",
    "$$ \\begin{aligned} \\log f(Y \\mid X, \\beta)p(\\beta) &= \\log \\left[ \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ 2b } \\right) \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 - \\frac{ \\lvert \\beta \\rvert }{ b } \\right) \\right] \\\\\n",
    "&= \\log \\left[ \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ 2b } \\right) \\right] - \\left( \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ \\lvert \\beta \\rvert }{ b } \\right) \\end{aligned} $$\n",
    "\n",
    "We want to maximize the posterior, this means: $$ \\begin{aligned} \\arg\\max_\\beta , f(\\beta \\mid X, Y) &= \\arg\\max_\\beta , \\log \\left[ \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ 2b } \\right) \\right] - \\left( \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ \\lvert \\beta \\rvert }{ b } \\right) \\ \\end{aligned} $$\n",
    "\n",
    "Since we are taking the difference of two values, the maximum of this value is the equivalent to taking the difference of the second value in terms of $\\beta$. This results in:\n",
    "\n",
    "$$ \\begin{aligned} &= \\arg\\min_\\beta , \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ \\lvert \\beta \\rvert }{ b } \\\\ \n",
    "&= \\arg\\min_\\beta , \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ 1 }{ b } \\sum_{j = 1}^{p} \\lvert \\beta_j \\rvert \\\\\n",
    "&= \\arg\\min_\\beta , \\frac{ 1 }{ 2\\sigma^2 } \\left( \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ 2\\sigma^2 }{ b } \\sum_{j = 1}^{p} \\lvert \\beta_j \\rvert \\right) \\end{aligned} $$\n",
    "\n",
    "By letting $\\lambda = 2\\sigma^2/b$, we can see that we end up with:\n",
    "\n",
    "$$ \\begin{aligned} &= \\arg\\min_\\beta , \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\lambda \\sum_{j = 1}^{p} \\lvert \\beta_j \\rvert \\\\\n",
    "&= \\arg\\min_\\beta , \\text{RSS} + \\lambda \\sum_{j = 1}^{p} \\lvert \\beta_j \\rvert \\end{aligned} $$\n",
    "\n",
    "which we know is the Lasso from Equation 6.7 in the book. Thus we know that when the posterior comes from a Laplace distribution with mean zero and common scale parameter $b$, the mode for $\\beta$ is given by the Lasso solution when $\\lambda = 2\\sigma^2 / b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7d'></a>\n",
    "**$(d)$ Now assume the following prior for $\\beta: \\beta_1, \\dots , \\beta_p$ are independent and identically distributed according to a normal distribution with mean zero and variance c. Write out the posterior for $\\beta$ in this setting.**\n",
    "\n",
    "**Answer:**\\\n",
    "The posterior distributed according to Normal distribution with mean 0 and variance $c$ is:\n",
    "\n",
    "$$ \\begin{aligned} f(\\beta \\mid X, Y) \\propto f(Y \\mid X, \\beta) p(\\beta \\mid X) = f(Y \\mid X, \\beta) p(\\beta) \\end{aligned} $$\n",
    "\n",
    "Our probability distribution function then becomes: $$ p(\\beta) = \\prod_{i = 1}^{p} p(\\beta_i) = \\prod_{i = 1}^{p} \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\exp \\left( - \\frac{ \\beta_i^2 }{ 2c } \\right) = \\left( \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\right)^p \\exp \\left( - \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) $$\n",
    "\n",
    "Substituting our values from (a) and our density function gives us:\n",
    "\n",
    "$$ \\begin{aligned} f(Y \\mid X, \\beta)p(\\beta) &= \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 \\right) \\left( \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\right)^p \\exp \\left( - \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\\\\n",
    "&= \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\right)^p \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 - \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7e'></a>\n",
    "**$(e)$ Argue that the ridge regression estimate is both the mode and the mean for $\\beta$ under this posterior distribution.**\n",
    "\n",
    "**Answer:**\\\n",
    "Like from part c, showing that the Ridge Regression estimate for $\\beta$ is the mode and mean under this posterior distribution is the same thing as showing that the most likely value for $\\beta$ is given by the lasso solution with a certain $\\lambda$.\n",
    "\n",
    "We can do this by taking our likelihood and posterior and showing that it can be reduced to the canonical Ridge Regression Equation 6.5 from the book.\n",
    "\n",
    "Let's start by simplifying it by taking the logarithm of both sides:\n",
    "\n",
    "Once again, we can take the logarithm of both sides to simplify it:\n",
    "\n",
    "$$ \\begin{aligned} \\log f(Y \\mid X, \\beta)p(\\beta) &= \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\right)^p \\exp \\left( - \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 - \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\\\\n",
    "&= \\log \\left[ \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\right)^p \\right] - \\left( \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\end{aligned} $$\n",
    "\n",
    "We want to maximize the posterior, this means: $$ \\begin{aligned} \\arg\\max_\\beta , f(\\beta \\mid X, Y) &= \\arg\\max_\\beta , \\log \\left[ \\left( \\frac{ 1 }{ \\sigma \\sqrt{2\\pi} } \\right)^n \\left( \\frac{ 1 }{ \\sqrt{ 2c\\pi } } \\right)^p \\right] - \\left( \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\end{aligned} $$\n",
    "\n",
    "Since we are taking the difference of two values, the maximum of this value is the equivalent to taking the difference of the second value in terms of $\\beta$. This results in:\n",
    "\n",
    "$$ \\begin{aligned} &= \\arg\\min_\\beta , \\left( \\frac{ 1 }{ 2\\sigma^2 } \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ 1 }{ 2c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\\\\n",
    "&= \\arg\\min_\\beta , \\left( \\frac{ 1 }{ 2\\sigma^2 } \\right) \\left( \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\frac{ \\sigma^2 }{ c } \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\end{aligned} $$\n",
    "\n",
    "By letting $\\lambda = \\sigma^2/ c$, we end up with:\n",
    "\n",
    "$$ \\begin{aligned} &= \\arg\\min_\\beta , \\left( \\frac{ 1 }{ 2\\sigma^2 } \\right) \\left( \\sum_{i = 1}^{n} \\left[ Y_i - (\\beta_0 + \\sum_{j = 1}^{p} \\beta_j X_{ij}) \\right]^2 + \\lambda \\sum_{i = 1}^{p} \\beta_i^2 \\right) \\\\\n",
    "&= \\arg\\min_\\beta , \\text{RSS} + \\lambda \\sum_{i = 1}^{p} \\beta_i^2 \\end{aligned} $$\n",
    "\n",
    "which we know is the Ridge Regression from Equation 6.5 in the book. Thus we know that when the posterior comes from a normal distribution with mean zero and variance $c$, the mode for $\\beta$ is given by the Ridge Regression solution when $\\lambda = \\sigma^2 / c$. Since the posterior is Gaussian, we also know that it is the posterior mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
